{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Notebook: Prepare the training samples from raw data\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The <strong>data treatment</strong>, previous to the network training, it is a very important part when it comes to Machine Learning analyses. There are infinite ways to represent the given information and find the optimal one is often tricky.\n",
    "\n",
    "It is also important to point out that the performance of the architecture will have a huge dependence on how the samples are presented.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Data format\n",
    "\n",
    "\n",
    "### 1.1 Raw Data\n",
    "\n",
    "The events available for the challenge are given in a .csv file with all the information needed for the DNN training. With the exception of the heading, each row corresponds to a single event of the sample.\n",
    "\n",
    "Since we are working with Keras (and Keras is built with NumPy) we will need to transform the rows of the CSV file into NumPy arrays that the DNN can read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading the data\n",
    "\n",
    "The package used to read the data first is [pandas](https://pandas.pydata.org/).\n",
    "\n",
    "By means of the ```read_csv(file, delimiter)``` method we can load all the information of the CSV file into a ```pandas.DataFrame``` object that we can access easily.\n",
    "\n",
    "We can take a look then to the data:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "data = pandas.read_csv(\"training.csv\", delimiter = ',') # DataFrame object with al the info of the csv\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And explore a little bit the information we have i.e. number of events, variables... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print general information about the data:\n",
    "print ('Size of data: {}'.format(data.shape))\n",
    "print ('Number of events: {}'.format(data.shape[0]))\n",
    "print ('Number of columns: {}'.format(data.shape[1]))\n",
    "\n",
    "print ('\\nList of features in dataset:')\n",
    "\n",
    "for col in data.columns:\n",
    "    print (col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that ```EventId```, ```Weight``` and ```Label``` are not training features but control variables, because they have no physical information. Therefore, we exclude them in the NumPy array creation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select the features with physical information:\n",
    "features = [h for h in list(data) if h not in ['EventId', 'Weight', 'Label']] \n",
    "print(features[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note: -999.0 values are missing or meaningless inputs in the data. They do not provide any physical information. For example, in any event with only one jet ```PRI_jet_subleading_pt```, ```PRI_jet_subleading_eta``` or ```PRI_jet_subleading_phi``` would have -999.0 value since there is no jet  where the kinematics can be obtained from.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Physical features representation\n",
    "\n",
    "Before training the network, it is very important to study the physical features we are going to use. This information could give us ideas of how to represent the data or even design the ML algorithm. \n",
    "\n",
    "The distributions of all the physical features are studied. We import the [NumPy](http://www.numpy.org/) package to use ```numpy.array``` structures and also [Matplotlib](https://matplotlib.org/) to generate the histograms.\n",
    "\n",
    "One histogram is created for each physical variable, with signal and background events separately. All -999.0 values are excluded from the plotting since they do not provide any physical nor relevant information. All histograms are saved in the ```Histograms/``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if not os.path.exists('Histograms'):\n",
    "    os.makedirs('Histograms')\n",
    "\n",
    "### Loop over the features\n",
    "for feature in features:\n",
    "    \n",
    "    print(\">>> Plotting \" + feature + \" histogram...\")\n",
    "    \n",
    "    # Signal and background values\n",
    "    signal_values = list( data.loc[data['Label'] == 's', feature] )\n",
    "    background_values = list( data.loc[data['Label'] == 'b', feature] )\n",
    "    \n",
    "    signal_values = list(filter(lambda x: x != -999.0, signal_values))\n",
    "    background_values = list(filter(lambda x: x != -999.0, background_values))\n",
    "    \n",
    "    # Define the histogram binning\n",
    "    xmin = min(signal_values + background_values) \n",
    "    xmax = max(signal_values + background_values)\n",
    "    binning = np.linspace(xmin, xmax, 61) \n",
    "    \n",
    "    # Plot and save the histogram\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize =(6,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.margins(x = 0)\n",
    "    \n",
    "    ax.hist(background_values, bins = binning, color = 'k', alpha = 0.4, histtype = 'stepfilled', \n",
    "             linewidth = 1, edgecolor = 'k', label = 'Background', density = True)\n",
    "    ax.hist(signal_values, bins = binning, color = 'r', alpha = 0.3, histtype = 'stepfilled', \n",
    "             linewidth = 1, edgecolor = 'r', label = 'Signal', density = True)\n",
    "    ax.legend(loc = 'best', fontsize = 10)\n",
    "    ax.set_xlabel(feature, fontsize = 14)\n",
    "    ax.set_ylabel('Event density', fontsize = 14)\n",
    "    ax.tick_params(axis='both', which='both', direction='in', \n",
    "                   bottom=True, top=True, left=True, right=True, labelsize=12)\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0), useMathText = True)\n",
    "    fig.savefig('Histograms/'+feature+'_histo.png', dpi = 600)\n",
    "    plt.close('all')\n",
    "          \n",
    "    print(\"    Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "histograms = glob.glob(\"Histograms/*.png\")\n",
    "\n",
    "for histo in histograms:\n",
    "    i = Image.open(histo)\n",
    "    i = i.resize((400,400))\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From raw samples to training datasets\n",
    "\n",
    "In this part of the notebook, it is explained how to get the samples ready for the training. The steps are the following:\n",
    "\n",
    "- **4.1**  Definition of train, development and test datasets\n",
    "\n",
    "- **4.2**  Normalization\n",
    "\n",
    "- **4.3**  NumPy array creation\n",
    "\n",
    "- **4.4**  Class balance\n",
    "\n",
    "- **4.5**  Shuffling\n",
    "\n",
    "- **4.6**  Save variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definition of train and test datasets\n",
    "\n",
    "The first thing to do is select the events that are going to be used for training the DNN (train dataset) and testing its final performance once trained (test dataset).\n",
    "\n",
    "Since both development and test datasets are used to evaluate the DNN performance, it is mandatory to have a proper class balance in them i.e. having the same number of signal and background events. If not, the results obtained from those datasets would not reflect the real DNN efficiency.\n",
    "\n",
    "Let's see how many events of each kind are stored in the csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two separate pandas dataframes for signal and background events:\n",
    "signal_data = data.loc[data['Label'] == 's']\n",
    "bkg_data = data.loc[data['Label'] == 'b']\n",
    "\n",
    "print ('Number of total signal events: {}'.format(signal_data.shape[0]))\n",
    "print ('Number of total background events: {}'.format(bkg_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakesig = signal_data.loc[signal_data['PRI_jet_subleading_eta'] == -999.00]\n",
    "fakebkg = bkg_data.loc[bkg_data['PRI_jet_subleading_eta'] == -999.00]\n",
    "print(fakesig.shape[0])\n",
    "print(fakebkg.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a total of 164333 samples are background events, only 85667 events are hadronic decays of the Higgs boson. This means that we will have only ~34% signal events of the entire dataset for training and testing the DNN.\n",
    "\n",
    "As said above, both development and test datasets must have 50%-50% signal-background balance to obtain reliable results and (for the same reason) no event must be duplicated there. We build those datasets first by taking 5000 events of each kind for each one of them. The remaining events will compoung the train dataset.\n",
    "\n",
    "Each dataset will have associated one ```pandas.DataFrame``` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 5000 events from each class for both the development and test datasets:\n",
    "n_test = 5000\n",
    "\n",
    "signal_test = signal_data[:n_test]\n",
    "bkg_test = bkg_data[:n_test] \n",
    "test = pandas.concat([signal_test, bkg_test]) # final test dataset\n",
    "\n",
    "# The remaining events will compound the training dataframe:\n",
    "\n",
    "signal_train = signal_data[n_test:]\n",
    "bkg_train = bkg_data[n_test:]\n",
    "train = pandas.concat([signal_train, bkg_train]) # final train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normalization\n",
    "\n",
    "Having values of different features taking wildly different ranges could make the learning process harder. A good practice is to do the so-called **feature-wise normalization**, forcing the feature distributions to be centered at 0 while having standard deviations equal to the unity.\n",
    "\n",
    "To have the values properly normalized we could just substract the mean value for each feature, and divide by its standard deviation. So, for each sample <em>i</em> and feature <em>j</em> value $x^{(i)}_j$ we apply:\n",
    "\n",
    "$$ x'^{(i)}_j = \\dfrac{x^{(i)}_j - \\bar{x}_j}{\\sigma (x_j)}$$\n",
    "\n",
    "being the mean value $\\bar{x}_j$ of feature *f* defined as\n",
    "\n",
    "$$\\bar{x}_j = \\dfrac{1}{N}\\sum_{i = 1}^{N} x^{(i)}_j$$\n",
    "\n",
    "and its standard deviation $\\sigma (x_j)$ defined as \n",
    "\n",
    "$$\\sigma(x_j) = \\sqrt{\\dfrac{\\sum_{i = 1}^{N} (x^{(i)}_j - \\bar{x}_j)}{N-1}}$$  \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "> In addition, there are some **IMPORTANT FACTS TO REMEMBER IF THE INPUT IS NORMALIZED** that must be rigurously taken into account:\n",
    "> - Both $\\bar{x}_j$ and $\\sigma (x_j)$ values **must be computed ONLY with the samples of the train dataset**. Remember that development and test datasets are just to measure performance and should not be used for nothing more.\n",
    "> - Although $\\bar{x}_j$ and $\\sigma (x_j)$ are computed with the train dataset, **all the datasets must be normalized in the same way**. The DNN is trained to classify samples of the same kind of those that have been used to compute its weights.\n",
    "> - -999.0 values are meaningless or missing inputs. The DNN is supossed to learn by itself that this value means exactly that, and must be the same for each feature. For these reason, **they must not enter in the $\\bar{x}_j$ and $\\sigma (x_j)$ computations nor be normalized either**. They just have to be left this way.\n",
    "\n",
    "We run over the events of the train dataset and we save the meand and standar deviation values for each one of the features in a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "means = {} # dict with the means of every feature\n",
    "stds = {} # dict with the standard deviations of every feature\n",
    "\n",
    "for feature in features:\n",
    "    true_values = train.loc[train[feature] != -999.0, feature] # We exclude -999.0 values\n",
    "    means[feature] = true_values.values.mean()\n",
    "    stds[feature] = true_values.values.std()\n",
    "    \n",
    "for feature in features:\n",
    "    print('> ' + feature)\n",
    "    print('mean:', means[feature])\n",
    "    print('std:', stds[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Input creation\n",
    "\n",
    "The DNN will take plain vectors $\\vec{x}^{(i)}$ as an input. These are given to the network as a matrix of the form\n",
    "\n",
    "$$\\vec{X} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{x}^{(1)} \\\\\n",
    "\\vec{x}^{(2)} \\\\\n",
    "... \\\\\n",
    "\\vec{x}^{(\\text{nEventos})}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "in which each one of the $x^{(i)}$ vectors is identified with an event of the train dataset. Therefore, the columns of this matrix will be asocciated with one feature each. That is \n",
    "\n",
    "$$\\vec{X} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{x}^{(1)}_1 & \\vec{x}^{(1)}_2 & ... & \\vec{x}^{(1)}_{\\text{nFeatures}} \\\\\n",
    "\\vec{x}^{(2)}_1 & \\vec{x}^{(2)}_2 & ... & \\vec{x}^{(2)}_{\\text{nFeatures}} \\\\\n",
    "... & ... & ...& ... \\\\\n",
    "\\vec{x}^{(\\text{nEvents})}_1 & \\vec{x}^{(\\text{nEvents})}_2 & ... &\\vec{x}^{(\\text{nEvents})}_{\\text{nFeatures}} \\\\\n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "  \n",
    "  \n",
    "Appart from the sample matrices $\\vec{X}$ we will need another vector $\\vec{y}$ to let the DNN know if each one of the $\\vec{x}^{(i)}$ vectors corresponds with a signal or a background event. This vector is called label vector and its elements will be either 0 (for background events) or 1 (for signal events):\n",
    "\n",
    "$$\\vec{y} = \n",
    "\\begin{pmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "... \\\\\n",
    "y_{(\\text{nEvents})}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "These two ingredients, sample matrix $\\vec{X}$ an label vector $\\vec{y}$, are the inputs of the network, and we will need to create them for train, develop and test datasets.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Since Keras is build with NumPy, the vectors will need to be defined as ```numpy.array``` objects. To create them, we define first a python function that will receive as an input a ```pandas.DataFrame``` object with the samples, a list with the features we want to include, and two python dictioraries with the mean and standard deviation values (created in the step before). The output of this function will be two ```numpy.array```'s: the sample matrix and the label vector, already normalized.\n",
    "\n",
    "We use this function to create the inputs of the train, development and test dataset.\n",
    "\n",
    "\n",
    "> **Note** that this function contains a loop that runs over all the events in the ```pandas.DataFrame``` given as the input so it will take time to create the vectors. (~1 hour and a half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getXandY(df, fnames, mean_values, std_values):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the sample matrix X and the label vector y constructed with:\n",
    "    - The events of df (pandas.DataFrame)\n",
    "    - The features indicated in fnames (list)\n",
    "    - The mean indicated in mean_values (dictionary)\n",
    "    - The standard deviation indicated in std_values (dictionary)\n",
    "    \"\"\"\n",
    "\n",
    "    progress = [int(i*float(len(df.index.values))) for i in np.linspace(0, 1, 101)]\n",
    "    \n",
    "    X = np.zeros(shape = (df.shape[0], len(fnames))) # empty sample matrix X (to be filled)\n",
    "    Y = np.zeros(shape = (df.shape[0], 1)) # empty label vector y (to be filled)\n",
    "\n",
    "    # Loop over dataset\n",
    "    for i,idx in enumerate(df.index.values):\n",
    "        for j,feature in enumerate(fnames):\n",
    "            \n",
    "            # (i: event)\n",
    "            # (j: feature)\n",
    "        \n",
    "            # X filling:\n",
    "            value = df.loc[idx][feature]\n",
    "            if (value == -999.0):\n",
    "                X[i][j] = -5.0\n",
    "            else: \n",
    "                X[i][j] = (value - mean_values[feature])/std_values[feature]\n",
    "            \n",
    "            # y_test filling:\n",
    "            if df.loc[idx]['Label'] == 's':\n",
    "                Y[i][0] = 1.0\n",
    "            else:\n",
    "                Y[i][0] = 0.0\n",
    "                \n",
    "        if (i in progress):\n",
    "            print(\"Progress: \"+ str(i)+\"/\"+str(len(df.index.values)) +\" completed\")\n",
    "                \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train dataset:\n",
    "x_train, y_train = getXandY(train, features, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset:\n",
    "x_test, y_test = getXandY(test, features, means, stds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Class balance\n",
    "\n",
    "There is one important thing to take into account when dealing with ML problems, which is the class balance in the datasets. It was mentioned before that development and test datasets must have the same number of signal/background events to give reliable measurements of performance.\n",
    "\n",
    "For example, suppose that a bad trained DNN is very efficient in identifying signal events but classifies background events randomly. If its performance is measured with a dataset with 80% signal events our (mistaken) perception would be that this DNN is correctly trained. \n",
    "\n",
    "And for the same reason a dataset used to measure the DNN performance must not have duplicates.\n",
    "\n",
    "Class balance in development and test datasets is adressed by taken the same number of signal and background events from the available data. But what happen with the train dataset which is used to optimize the DNN that takes the remaining events? \n",
    "\n",
    "Let's see how is the class balance there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class balance in training set:\n",
    "s_train = np.count_nonzero(y_train == 1.0)\n",
    "b_train = np.count_nonzero(y_train == 0.0)\n",
    "print(\"Number of signal events in training set: \" + \"{}\".format(s_train))\n",
    "print(\"Number of background events in training set: \" + \"{}\".format(b_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the number of signal events is much lower than the number of background events. Class unbalance in the train dataset is not as dramatic as in development and test ones but could affect the learning process. It may cause, among other things, that the network only learns how to classify one kind of event. In an extreme scenario, if a train dataset is full of events of one kind, the DNN would learn that the \"optimal\" way to have a good score is to always classify any given event in the most populated category.\n",
    "\n",
    "The best scenario is having a proper 50-50 class balance. There are to valid options to get it:\n",
    "- Change DNN configuration to apply a weight in the less populated class when training\n",
    "- Duplicate some events until matching the most populated class number\n",
    "\n",
    "The last one is the followed one in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_extra = []\n",
    "\n",
    "while(s_train < b_train): # repeat until we have our classes balanced\n",
    "    \n",
    "    for i,idx in enumerate(signal_train.index.values):\n",
    "        \n",
    "        if s_train >= b_train: break # if we have all the extra arrays we need we just quit\n",
    "        \n",
    "        x_extra = np.zeros(len(features))\n",
    "        \n",
    "        for j,feature in enumerate(features):\n",
    "            \n",
    "            value = signal_train.loc[idx][feature]\n",
    "            \n",
    "            if (value == -999.0):\n",
    "                x_extra[j] = -5.0\n",
    "            else: \n",
    "                x_extra[j] = (value - means[feature])/stds[feature]\n",
    "            \n",
    "        x_train_extra.append(x_extra)\n",
    "        s_train += 1\n",
    "        \n",
    "### Add this extra vectors to the existing training dataset:\n",
    "x_train = np.concatenate((x_train, np.array(x_train_extra)), axis = 0)\n",
    "y_train = np.concatenate((y_train, np.full(shape = (len(x_train_extra), 1), fill_value = 1.0)))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we check again the number of events of each class in the train dataset we can see that now it is properly balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print again class balance in training set:\n",
    "s_train = np.count_nonzero(y_train == 1.0)\n",
    "b_train = np.count_nonzero(y_train == 0.0)\n",
    "print(\"Number of signal events in training set: \" + \"{}\".format(s_train))\n",
    "print(\"Number of background events in training set: \" + \"{}\".format(b_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Shuffle the samples\n",
    "\n",
    "Finally, when we have our datasets prepared, it is mandatory to assure that the input in the DNN is correctly shuffled. Events of different classes have to be read by the DNN in an homogeneous way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "### Get the indexes of each dataset:\n",
    "idx_train = np.arange(y_train.shape[0])\n",
    "idx_test = np.arange(y_test.shape[0])\n",
    "\n",
    "### Shuffle them:\n",
    "random.shuffle(idx_train)\n",
    "random.shuffle(idx_test)\n",
    "\n",
    "### And apply the new ordering to every input of each dataset:\n",
    "y_train = y_train[idx_train]\n",
    "x_train = x_train[idx_train]\n",
    "y_test = y_test[idx_test]\n",
    "x_test = x_test[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Save the variables\n",
    "\n",
    "As the sample creation process is long and also computationally expensive, it not advisable to create the inputs every time we want to train the DNN.\n",
    "\n",
    "For this reason, input variables are created once and saved in a pickle file that will allow to access them in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('DNNtraining_variables.p', 'wb') as file_:\n",
    "        pickle.dump([x_train, y_train, x_test, y_test], file_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
